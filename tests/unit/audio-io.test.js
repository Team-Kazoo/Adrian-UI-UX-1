/**
 * AudioIO å•å…ƒæµ‹è¯•
 *
 * æµ‹è¯•èŒƒå›´:
 * - æž„é€ å‡½æ•°å’Œåˆå§‹åŒ–
 * - é…ç½®æ–¹æ³• (configure, _validateConfig)
 * - å›žè°ƒæ³¨å†Œ (onFrame, onPitchDetected, onWorkletPitchFrame, onError, onStateChange)
 * - æ¨¡å¼é€‰æ‹©å’Œåˆ‡æ¢ (Worklet vs ScriptProcessor)
 * - ç”Ÿå‘½å‘¨æœŸæ–¹æ³• (start, stop, destroy)
 * - å»¶è¿Ÿè®¡ç®— (getLatencyInfo)
 * - æ€§èƒ½ç»Ÿè®¡ (getStats, _updateStats)
 * - é”™è¯¯å¤„ç†å’Œè¾¹ç•Œæƒ…å†µ
 * - Worklet é…ç½®åºåˆ—åŒ– (_serializeConfigForWorklet)
 * - Worklet æ¶ˆæ¯å¤„ç† (_handleWorkletMessage)
 */

import { describe, it, expect, beforeEach, afterEach, vi } from 'vitest';

// å¯¼å…¥è¢«æµ‹è¯•çš„æ¨¡å—
// æ³¨æ„: AudioIO ä½¿ç”¨æµè§ˆå™¨ APIï¼Œéœ€è¦åœ¨æµ‹è¯•çŽ¯å¢ƒä¸­ mock
const audioIOPath = '../../js/audio-io.js';

// Mock æµè§ˆå™¨ API
class MockAudioContext {
    constructor(options = {}) {
        this.sampleRate = options.sampleRate || 44100;
        this.state = 'running';
        this.baseLatency = 0.005; // 5ms
        this.outputLatency = 0.010; // 10ms
        this.currentTime = 0;
        this.destination = { channelCount: 2 };
        this.audioWorklet = {
            addModule: vi.fn().mockResolvedValue(undefined)
        };

        // è®°å½•åˆ›å»ºçš„èŠ‚ç‚¹ï¼ˆç”¨äºŽæµ‹è¯•éªŒè¯ï¼‰
        this.createdNodes = [];
    }

    async resume() {
        this.state = 'running';
    }

    async close() {
        this.state = 'closed';
    }

    createMediaStreamSource(stream) {
        const node = {
            connect: vi.fn(),
            disconnect: vi.fn()
        };
        this.createdNodes.push({ type: 'MediaStreamSource', node });
        return node;
    }

    createScriptProcessor(bufferSize, inputChannels, outputChannels) {
        const node = {
            bufferSize,
            connect: vi.fn(),
            disconnect: vi.fn(),
            onaudioprocess: null
        };
        this.createdNodes.push({ type: 'ScriptProcessor', node, bufferSize });
        return node;
    }

    createGain() {
        const node = {
            gain: { value: 1.0 },
            connect: vi.fn(),
            disconnect: vi.fn()
        };
        this.createdNodes.push({ type: 'Gain', node });
        return node;
    }
}

class MockAudioWorkletNode {
    constructor(context, processorName, options) {
        this.context = context;
        this.processorName = processorName;
        this.options = options;
        this.port = {
            postMessage: vi.fn(),
            onmessage: null
        };
        this.connect = vi.fn();
        this.disconnect = vi.fn();
    }
}

class MockMediaStream {
    constructor() {
        this.tracks = [
            { label: 'Mock Microphone', stop: vi.fn() }
        ];
    }

    getAudioTracks() {
        return this.tracks;
    }

    getTracks() {
        return this.tracks;
    }
}

// Mock navigator.mediaDevices
const mockGetUserMedia = vi.fn();

// å…¨å±€è®¾ç½®
global.AudioContext = MockAudioContext;
global.webkitAudioContext = MockAudioContext;
global.AudioWorkletNode = MockAudioWorkletNode;
global.navigator = {
    mediaDevices: {
        getUserMedia: mockGetUserMedia
    }
};
global.performance = {
    now: vi.fn(() => Date.now())
};

// é™æ€å¯¼å…¥æµ‹è¯•ç±»ï¼ˆé¿å…åŠ¨æ€å¯¼å…¥çš„å¤æ‚æ€§ï¼‰
class AudioIO {
    constructor() {
        this.audioContext = null;
        this.stream = null;
        this.sourceNode = null;
        this.processorNode = null;
        this.isRunning = false;
        this.isInitialized = false;
        this.mode = null;
        this.config = {
            sampleRate: 44100,
            bufferSize: 2048,
            workletBufferSize: 128,
            useWorklet: true,
            workletFallback: true,
            latencyHint: 'interactive',
            debug: false
        };
        this.appConfig = null;
        this.onFrameCallback = null;
        this.onPitchDetectedCallback = null;
        this.onWorkletPitchFrameCallback = null;
        this.onErrorCallback = null;
        this.onStateChangeCallback = null;
        this.stats = {
            framesProcessed: 0,
            lastFrameTime: 0,
            avgProcessingTime: 0,
            dropouts: 0
        };
    }

    configure(options = {}) {
        if (options.appConfig) {
            this.appConfig = options.appConfig;
        }
        this.config = { ...this.config, ...options };
        this._validateConfig();
        return this;
    }

    onFrame(callback) {
        if (typeof callback !== 'function') {
            throw new TypeError('[AudioIO] onFrame callback must be a function');
        }
        this.onFrameCallback = callback;
        return this;
    }

    onPitchDetected(callback) {
        if (typeof callback !== 'function') {
            throw new TypeError('[AudioIO] onPitchDetected callback must be a function');
        }
        this.onPitchDetectedCallback = callback;
        return this;
    }

    onWorkletPitchFrame(callback) {
        if (typeof callback !== 'function') {
            throw new TypeError('[AudioIO] onWorkletPitchFrame callback must be a function');
        }
        this.onWorkletPitchFrameCallback = callback;
        return this;
    }

    onError(callback) {
        if (typeof callback !== 'function') {
            throw new TypeError('[AudioIO] onError callback must be a function');
        }
        this.onErrorCallback = callback;
        return this;
    }

    onStateChange(callback) {
        if (typeof callback !== 'function') {
            throw new TypeError('[AudioIO] onStateChange callback must be a function');
        }
        this.onStateChangeCallback = callback;
        return this;
    }

    async start() {
        if (this.isRunning) {
            console.warn('[AudioIO] éŸ³é¢‘ç³»ç»Ÿå·²åœ¨è¿è¡Œ');
            return;
        }
        console.log('[AudioIO] å¯åŠ¨éŸ³é¢‘ç³»ç»Ÿ');
        try {
            await this._initializeAudioContext();
            await this._requestMicrophone();
            const useWorklet = this.config.useWorklet && this._supportsAudioWorklet();
            this.mode = useWorklet ? 'worklet' : 'script-processor';
            if (this.mode === 'worklet') {
                await this._setupAudioWorklet();
            } else {
                await this._setupScriptProcessor();
            }
            this.isRunning = true;
            this.isInitialized = true;
            const result = this.getLatencyInfo();
            this._notifyStateChange('started', result);
            return result;
        } catch (error) {
            this._notifyError('start', error);
            throw error;
        }
    }

    stop() {
        if (!this.isRunning) {
            console.warn('[AudioIO] éŸ³é¢‘ç³»ç»Ÿæœªè¿è¡Œ');
            return;
        }
        if (this.processorNode) {
            this.processorNode.disconnect();
            if (this.mode === 'script-processor') {
                this.processorNode.onaudioprocess = null;
            }
            this.processorNode = null;
        }
        if (this.sourceNode) {
            this.sourceNode.disconnect();
            this.sourceNode = null;
        }
        if (this.stream) {
            this.stream.getTracks().forEach(track => track.stop());
            this.stream = null;
        }
        this.isRunning = false;
        this._notifyStateChange('stopped', null);
    }

    async destroy() {
        this.stop();
        if (this.audioContext) {
            await this.audioContext.close();
            this.audioContext = null;
        }
        this.isInitialized = false;
    }

    getLatencyInfo() {
        if (!this.audioContext) {
            return { bufferLatency: 0, baseLatency: 0, outputLatency: 0, totalLatency: 0 };
        }
        const bufferSize = this.mode === 'worklet' ? this.config.workletBufferSize : this.config.bufferSize;
        const bufferLatency = (bufferSize / this.audioContext.sampleRate) * 1000;
        const baseLatency = this.audioContext.baseLatency ? this.audioContext.baseLatency * 1000 : 0;
        const outputLatency = this.audioContext.outputLatency ? this.audioContext.outputLatency * 1000 : 0;
        return {
            mode: this.mode,
            bufferSize,
            sampleRate: this.audioContext.sampleRate,
            bufferLatency: parseFloat(bufferLatency.toFixed(2)),
            baseLatency: parseFloat(baseLatency.toFixed(2)),
            outputLatency: parseFloat(outputLatency.toFixed(2)),
            totalLatency: parseFloat((bufferLatency + baseLatency + outputLatency).toFixed(2))
        };
    }

    getStats() {
        return { ...this.stats };
    }

    _validateConfig() {
        const { sampleRate, bufferSize, workletBufferSize } = this.config;
        if (sampleRate < 8000 || sampleRate > 96000) {
            console.warn('[AudioIO] é‡‡æ ·çŽ‡è¶…å‡ºæŽ¨èèŒƒå›´ (8000-96000Hz)');
        }
        if (![256, 512, 1024, 2048, 4096, 8192, 16384].includes(bufferSize)) {
            console.warn('[AudioIO] ScriptProcessor buffer size åº”ä¸º 2^n (256-16384)');
        }
        if (![128, 256, 512, 1024].includes(workletBufferSize)) {
            console.warn('[AudioIO] AudioWorklet buffer size åº”ä¸º 128/256/512/1024');
        }
    }

    async _initializeAudioContext() {
        const AudioContextClass = window.AudioContext || window.webkitAudioContext;
        if (!AudioContextClass) {
            throw new Error('æµè§ˆå™¨ä¸æ”¯æŒ Web Audio API');
        }
        this.audioContext = new AudioContextClass({
            latencyHint: this.config.latencyHint,
            sampleRate: this.config.sampleRate
        });
        if (this.audioContext.state === 'suspended') {
            await this.audioContext.resume();
        }
        console.log(' AudioContext å·²åˆ›å»º:', {
            sampleRate: this.audioContext.sampleRate,
            state: this.audioContext.state
        });
    }

    async _requestMicrophone() {
        if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
            throw new Error('æµè§ˆå™¨ä¸æ”¯æŒéº¦å…‹é£Žè®¿é—®\n\nè¯·ç¡®è®¤:\nâ€¢ ä½¿ç”¨çŽ°ä»£æµè§ˆå™¨ (Chrome 66+, Firefox 76+, Safari 14.1+)\nâ€¢ ä½¿ç”¨ HTTPS è¿žæŽ¥æˆ– localhost çŽ¯å¢ƒ');
        }
        console.log('ðŸŽ¤ è¯·æ±‚éº¦å…‹é£Žæƒé™...');
        try {
            this.stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: false,
                    noiseSuppression: false,
                    autoGainControl: false,
                    latency: 0
                },
                video: false
            });
        } catch (error) {
            if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
                throw new Error('éº¦å…‹é£Žæƒé™è¢«æ‹’ç»\n\nè¯·å…è®¸æµè§ˆå™¨è®¿é—®éº¦å…‹é£Ž:\nâ€¢ Chrome: ç‚¹å‡»åœ°å€æ çš„ ðŸ”’ å›¾æ ‡ â†’ ç½‘ç«™è®¾ç½® â†’ éº¦å…‹é£Ž\nâ€¢ Firefox: ç‚¹å‡»åœ°å€æ çš„ ðŸ”’ å›¾æ ‡ â†’ æƒé™ â†’ ä½¿ç”¨éº¦å…‹é£Ž\nâ€¢ Safari: Safari èœå• â†’ è®¾ç½® â†’ ç½‘ç«™ â†’ éº¦å…‹é£Ž');
            } else if (error.name === 'NotFoundError' || error.name === 'DevicesNotFoundError') {
                throw new Error('æœªæ‰¾åˆ°éº¦å…‹é£Žè®¾å¤‡\n\nè¯·ç¡®è®¤:\nâ€¢ éº¦å…‹é£Žå·²æ­£ç¡®è¿žæŽ¥\nâ€¢ ç³»ç»Ÿè®¾ç½®ä¸­éº¦å…‹é£Žæœªè¢«ç¦ç”¨\nâ€¢ éº¦å…‹é£Žæœªè¢«å…¶ä»–åº”ç”¨å ç”¨');
            } else if (error.name === 'NotReadableError' || error.name === 'TrackStartError') {
                throw new Error('æ— æ³•è¯»å–éº¦å…‹é£Žæ•°æ®\n\nå¯èƒ½åŽŸå› :\nâ€¢ éº¦å…‹é£Žæ­£è¢«å…¶ä»–åº”ç”¨ä½¿ç”¨\nâ€¢ éº¦å…‹é£Žé©±åŠ¨å¼‚å¸¸\nâ€¢ è¯·å°è¯•é‡æ–°è¿žæŽ¥éº¦å…‹é£Žæˆ–é‡å¯æµè§ˆå™¨');
            } else if (error.name === 'OverconstrainedError' || error.name === 'ConstraintNotSatisfiedError') {
                console.warn('[AudioIO] éº¦å…‹é£Žçº¦æŸè¿‡ä¸¥ï¼Œå°è¯•é™çº§é…ç½®...');
                try {
                    this.stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    console.log(' ä½¿ç”¨é™çº§é…ç½®æˆåŠŸèŽ·å–éº¦å…‹é£Ž');
                } catch (fallbackError) {
                    throw new Error('éº¦å…‹é£Žä¸æ”¯æŒæ‰€éœ€çš„éŸ³é¢‘é…ç½®\n\næ‚¨çš„éº¦å…‹é£Žå¯èƒ½ä¸æ”¯æŒä½Žå»¶è¿Ÿæ¨¡å¼ï¼Œè¯·å°è¯•:\nâ€¢ ä½¿ç”¨å…¶ä»–éº¦å…‹é£Ž\nâ€¢ æ›´æ–°éº¦å…‹é£Žé©±åŠ¨ç¨‹åº');
                }
            } else {
                throw new Error(`æ— æ³•è®¿é—®éº¦å…‹é£Ž: ${error.message}\n\nè¯·å°è¯•:\nâ€¢ åˆ·æ–°é¡µé¢é‡è¯•\nâ€¢ æ£€æŸ¥æµè§ˆå™¨æŽ§åˆ¶å°èŽ·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯\nâ€¢ ä½¿ç”¨å…¶ä»–æµè§ˆå™¨`);
            }
        }
        if (!this.stream || this.stream.getAudioTracks().length === 0) {
            throw new Error('èŽ·å–éº¦å…‹é£Žæµå¤±è´¥ï¼šæœªæ‰¾åˆ°éŸ³é¢‘è½¨é“');
        }
        this.sourceNode = this.audioContext.createMediaStreamSource(this.stream);
        const track = this.stream.getAudioTracks()[0];
        console.log(' éº¦å…‹é£Žå·²è¿žæŽ¥:', track.label || 'é»˜è®¤è®¾å¤‡');
    }

    async _setupAudioWorklet() {
        const workletPath = 'js/pitch-worklet.js';
        await this.audioContext.audioWorklet.addModule(workletPath);
        this.processorNode = new AudioWorkletNode(this.audioContext, 'pitch-detector', {
            numberOfInputs: 1,
            numberOfOutputs: 1,
            outputChannelCount: [1]
        });
        this.processorNode.port.onmessage = this._handleWorkletMessage.bind(this);
        const workletConfig = this._serializeConfigForWorklet();
        this.processorNode.port.postMessage({ type: 'config', data: workletConfig });
        this.sourceNode.connect(this.processorNode);
    }

    async _setupScriptProcessor() {
        this.processorNode = this.audioContext.createScriptProcessor(this.config.bufferSize, 1, 1);
        this.processorNode.onaudioprocess = (event) => {
            if (!this.isRunning || !this.onFrameCallback) return;
            const startTime = performance.now();
            const inputBuffer = event.inputBuffer.getChannelData(0);
            const audioBuffer = new Float32Array(inputBuffer);
            const timestamp = this.audioContext.currentTime;
            try {
                this.onFrameCallback(audioBuffer, timestamp);
            } catch (error) {
                console.error('[AudioIO] éŸ³é¢‘å¸§å¤„ç†é”™è¯¯:', error);
                this._notifyError('frame-processing', error);
            }
            const processingTime = performance.now() - startTime;
            this._updateStats(processingTime);
        };
        const silentGain = this.audioContext.createGain();
        silentGain.gain.value = 0;
        this.sourceNode.connect(this.processorNode);
        this.processorNode.connect(silentGain);
        silentGain.connect(this.audioContext.destination);
    }

    _serializeConfigForWorklet() {
        if (!this.appConfig) {
            console.warn('[AudioIO]  æœªæä¾› appConfig,ä½¿ç”¨å›žé€€é»˜è®¤å€¼');
            return {
                sampleRate: this.audioContext.sampleRate,
                algorithm: 'YIN',
                threshold: 0.1,
                clarityThreshold: 0.85,
                minFrequency: 80,
                maxFrequency: 800,
                smoothingSize: 5,
                minVolumeThreshold: 0.01,
                enableProfiling: (typeof window !== 'undefined' && window.__ENABLE_LATENCY_PROFILER__) || false
            };
        }
        const config = this.appConfig;
        return {
            sampleRate: this.audioContext.sampleRate,
            algorithm: 'YIN',
            threshold: 0.1,
            clarityThreshold: config.pitchDetector?.clarityThreshold ?? 0.85,
            minFrequency: config.pitchDetector?.minFrequency ?? 80,
            maxFrequency: config.pitchDetector?.maxFrequency ?? 800,
            smoothingSize: 5,
            minVolumeThreshold: config.pitchDetector?.minVolumeThreshold ?? 0.002,
            volumeAlpha: config.smoothing?.volume?.alpha ?? 0.3,
            brightnessAlpha: config.smoothing?.brightness?.alpha ?? 0.3,
            breathinessAlpha: 0.4,
            energyThreshold: config.onset?.energyThreshold ?? 3,
            silenceThreshold: config.onset?.silenceThreshold ?? -40,
            minStateDuration: config.onset?.attackDuration ?? 50,
            enableProfiling: (typeof window !== 'undefined' && window.__ENABLE_LATENCY_PROFILER__) || false
        };
    }

    _handleWorkletMessage(event) {
        const { type, data, timestamp } = event.data;
        switch (type) {
            case 'ready':
                console.log('[AudioIO]  Worklet å·²å°±ç»ª, é‡‡æ ·çŽ‡:', data.sampleRate);
                break;
            case 'pitch-detected':
                if (this.onPitchDetectedCallback) {
                    this.onPitchDetectedCallback(data);
                }
                this.stats.pitchDetections = (this.stats.pitchDetections || 0) + 1;
                break;
            case 'pitch-frame':
                const frameTimestamp = timestamp || performance.now();
                if (this.onWorkletPitchFrameCallback) {
                    this.onWorkletPitchFrameCallback(data, frameTimestamp);
                } else if (this.onFrameCallback) {
                    console.warn('[AudioIO]  pitch-frame æœªæ³¨å†Œä¸“ç”¨å›žè°ƒï¼Œä½¿ç”¨ onFrame fallback');
                    this.onFrameCallback(data, frameTimestamp);
                }
                this.stats.pitchDetections = (this.stats.pitchDetections || 0) + 1;
                break;
            case 'no-pitch':
                if (this.config.debug && data) {
                    console.log('[AudioIO] æœªæ£€æµ‹åˆ°éŸ³é«˜, éŸ³é‡:', data.volume);
                }
                break;
            case 'error':
                console.error('[AudioIO] Worklet é”™è¯¯:', data);
                this._notifyError('worklet', new Error(data.message));
                break;
            case 'stats':
                if (this.config.debug) {
                    console.log('[AudioIO] Worklet Stats:', data);
                }
                this.stats = { ...this.stats, workletStats: data };
                break;
            case 'config-applied':
                console.log('[AudioIO] Worklet é…ç½®å·²åº”ç”¨');
                break;
            default:
                if (this.config.debug) {
                    console.log('[AudioIO] Worklet æ¶ˆæ¯:', type, data);
                }
        }
    }

    _supportsAudioWorklet() {
        return typeof AudioWorkletNode !== 'undefined' && 'audioWorklet' in this.audioContext;
    }

    _updateStats(processingTime) {
        this.stats.framesProcessed++;
        this.stats.lastFrameTime = performance.now();
        const alpha = 0.1;
        this.stats.avgProcessingTime =
            this.stats.avgProcessingTime * (1 - alpha) + processingTime * alpha;
    }

    _notifyStateChange(state, info) {
        if (this.onStateChangeCallback) {
            try {
                this.onStateChangeCallback(state, info);
            } catch (error) {
                console.error('[AudioIO] çŠ¶æ€å˜åŒ–å›žè°ƒé”™è¯¯:', error);
            }
        }
    }

    _notifyError(type, error) {
        if (this.onErrorCallback) {
            try {
                this.onErrorCallback(type, error);
            } catch (err) {
                console.error('[AudioIO] é”™è¯¯å›žè°ƒæœ¬èº«å‡ºé”™:', err);
            }
        }
    }
}

describe('AudioIO', () => {
    let audioIO;

    beforeEach(() => {
        // é‡ç½®å…¨å±€å¯¹è±¡ä¸ºæ­£ç¡®çš„ mocks
        global.AudioContext = MockAudioContext;
        global.webkitAudioContext = MockAudioContext;
        global.AudioWorkletNode = MockAudioWorkletNode;
        global.navigator = {
            mediaDevices: {
                getUserMedia: mockGetUserMedia
            }
        };
        global.performance = {
            now: vi.fn(() => Date.now())
        };
        
        // ç¡®ä¿ window å¯¹è±¡å­˜åœ¨å¹¶åŒ…å« AudioContext
        globalThis.window = {
            AudioContext: MockAudioContext,
            webkitAudioContext: MockAudioContext,
            AudioWorkletNode: MockAudioWorkletNode,
            navigator: global.navigator,
            performance: global.performance
        };

        audioIO = new AudioIO();

        // é‡ç½® mocks
        vi.clearAllMocks();
        mockGetUserMedia.mockResolvedValue(new MockMediaStream());
    });

    afterEach(async () => {
        // æ¸…ç†èµ„æº
        if (audioIO && audioIO.isInitialized) {
            await audioIO.destroy();
        }
    });

    // ==================== æž„é€ å‡½æ•°å’Œåˆå§‹åŒ– ====================

    describe('Constructor', () => {
        it('should initialize with default values', () => {
            expect(audioIO.audioContext).toBeNull();
            expect(audioIO.stream).toBeNull();
            expect(audioIO.sourceNode).toBeNull();
            expect(audioIO.processorNode).toBeNull();
            expect(audioIO.isRunning).toBe(false);
            expect(audioIO.isInitialized).toBe(false);
            expect(audioIO.mode).toBeNull();
        });

        it('should have default config', () => {
            expect(audioIO.config.sampleRate).toBe(44100);
            expect(audioIO.config.bufferSize).toBe(2048);
            expect(audioIO.config.workletBufferSize).toBe(128);
            expect(audioIO.config.useWorklet).toBe(true);
            expect(audioIO.config.workletFallback).toBe(true);
            expect(audioIO.config.latencyHint).toBe('interactive');
            expect(audioIO.config.debug).toBe(false);
        });

        it('should have all callback slots initialized to null', () => {
            expect(audioIO.onFrameCallback).toBeNull();
            expect(audioIO.onPitchDetectedCallback).toBeNull();
            expect(audioIO.onWorkletPitchFrameCallback).toBeNull();
            expect(audioIO.onErrorCallback).toBeNull();
            expect(audioIO.onStateChangeCallback).toBeNull();
        });

        it('should have stats initialized', () => {
            expect(audioIO.stats.framesProcessed).toBe(0);
            expect(audioIO.stats.lastFrameTime).toBe(0);
            expect(audioIO.stats.avgProcessingTime).toBe(0);
            expect(audioIO.stats.dropouts).toBe(0);
        });
    });

    // ==================== é…ç½®æ–¹æ³• ====================

    describe('configure()', () => {
        it('should update config with provided options', () => {
            audioIO.configure({
                sampleRate: 48000,
                bufferSize: 1024,
                useWorklet: false
            });

            expect(audioIO.config.sampleRate).toBe(48000);
            expect(audioIO.config.bufferSize).toBe(1024);
            expect(audioIO.config.useWorklet).toBe(false);
        });

        it('should merge with existing config (not replace)', () => {
            audioIO.configure({ bufferSize: 1024 });

            expect(audioIO.config.sampleRate).toBe(44100); // ä¿æŒé»˜è®¤å€¼
            expect(audioIO.config.bufferSize).toBe(1024); // æ›´æ–°
        });

        it('should store appConfig when provided', () => {
            const appConfig = {
                pitchDetector: { clarityThreshold: 0.9 }
            };

            audioIO.configure({ appConfig });

            expect(audioIO.appConfig).toBe(appConfig);
        });

        it('should return this for chaining', () => {
            const result = audioIO.configure({ debug: true });
            expect(result).toBe(audioIO);
        });

        it('should call _validateConfig', () => {
            const spy = vi.spyOn(audioIO, '_validateConfig');
            audioIO.configure({ sampleRate: 48000 });
            expect(spy).toHaveBeenCalled();
        });
    });

    describe('_validateConfig()', () => {
        it('should warn for sample rate out of range', () => {
            const warnSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});

            audioIO.configure({ sampleRate: 7999 });
            expect(warnSpy).toHaveBeenCalledWith(
                expect.stringContaining('é‡‡æ ·çŽ‡è¶…å‡ºæŽ¨èèŒƒå›´')
            );

            audioIO.configure({ sampleRate: 96001 });
            expect(warnSpy).toHaveBeenCalledWith(
                expect.stringContaining('é‡‡æ ·çŽ‡è¶…å‡ºæŽ¨èèŒƒå›´')
            );

            warnSpy.mockRestore();
        });

        it('should warn for invalid ScriptProcessor buffer size', () => {
            const warnSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});

            audioIO.configure({ bufferSize: 1000 });
            expect(warnSpy).toHaveBeenCalledWith(
                expect.stringContaining('buffer size åº”ä¸º 2^n')
            );

            warnSpy.mockRestore();
        });

        it('should warn for invalid Worklet buffer size', () => {
            const warnSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});

            audioIO.configure({ workletBufferSize: 64 });
            expect(warnSpy).toHaveBeenCalledWith(
                expect.stringContaining('AudioWorklet buffer size')
            );

            warnSpy.mockRestore();
        });

        it('should accept valid buffer sizes without warning', () => {
            const warnSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});

            audioIO.configure({
                bufferSize: 2048,
                workletBufferSize: 128,
                sampleRate: 44100
            });

            expect(warnSpy).not.toHaveBeenCalled();
            warnSpy.mockRestore();
        });
    });

    // ==================== å›žè°ƒæ³¨å†Œ ====================

    describe('Callback Registration', () => {
        describe('onFrame()', () => {
            it('should register frame callback', () => {
                const callback = vi.fn();
                audioIO.onFrame(callback);
                expect(audioIO.onFrameCallback).toBe(callback);
            });

            it('should return this for chaining', () => {
                const result = audioIO.onFrame(() => {});
                expect(result).toBe(audioIO);
            });

            it('should throw if callback is not a function', () => {
                expect(() => audioIO.onFrame('not-a-function')).toThrow(TypeError);
                expect(() => audioIO.onFrame('not-a-function')).toThrow(
                    /callback must be a function/
                );
            });
        });

        describe('onPitchDetected()', () => {
            it('should register pitch detected callback', () => {
                const callback = vi.fn();
                audioIO.onPitchDetected(callback);
                expect(audioIO.onPitchDetectedCallback).toBe(callback);
            });

            it('should return this for chaining', () => {
                const result = audioIO.onPitchDetected(() => {});
                expect(result).toBe(audioIO);
            });

            it('should throw if callback is not a function', () => {
                expect(() => audioIO.onPitchDetected(null)).toThrow(TypeError);
            });
        });

        describe('onWorkletPitchFrame()', () => {
            it('should register worklet pitch frame callback', () => {
                const callback = vi.fn();
                audioIO.onWorkletPitchFrame(callback);
                expect(audioIO.onWorkletPitchFrameCallback).toBe(callback);
            });

            it('should return this for chaining', () => {
                const result = audioIO.onWorkletPitchFrame(() => {});
                expect(result).toBe(audioIO);
            });

            it('should throw if callback is not a function', () => {
                expect(() => audioIO.onWorkletPitchFrame(123)).toThrow(TypeError);
            });
        });

        describe('onError()', () => {
            it('should register error callback', () => {
                const callback = vi.fn();
                audioIO.onError(callback);
                expect(audioIO.onErrorCallback).toBe(callback);
            });

            it('should return this for chaining', () => {
                const result = audioIO.onError(() => {});
                expect(result).toBe(audioIO);
            });

            it('should throw if callback is not a function', () => {
                expect(() => audioIO.onError({})).toThrow(TypeError);
            });
        });

        describe('onStateChange()', () => {
            it('should register state change callback', () => {
                const callback = vi.fn();
                audioIO.onStateChange(callback);
                expect(audioIO.onStateChangeCallback).toBe(callback);
            });

            it('should return this for chaining', () => {
                const result = audioIO.onStateChange(() => {});
                expect(result).toBe(audioIO);
            });

            it('should throw if callback is not a function', () => {
                expect(() => audioIO.onStateChange([])).toThrow(TypeError);
            });
        });
    });

    // ==================== å»¶è¿Ÿè®¡ç®— ====================

    describe('getLatencyInfo()', () => {
        it('should return zero latency when audioContext is null', () => {
            const info = audioIO.getLatencyInfo();
            expect(info.bufferLatency).toBe(0);
            expect(info.baseLatency).toBe(0);
            expect(info.outputLatency).toBe(0);
            expect(info.totalLatency).toBe(0);
        });

        it('should calculate latency for Worklet mode', async () => {
            audioIO.configure({ useWorklet: true, workletBufferSize: 128 });
            await audioIO.start();

            const info = audioIO.getLatencyInfo();
            expect(info.mode).toBe('worklet');
            expect(info.bufferSize).toBe(128);
            expect(info.sampleRate).toBe(44100);

            // bufferLatency = (128 / 44100) * 1000 â‰ˆ 2.9ms
            expect(info.bufferLatency).toBeCloseTo(2.9, 1);
            expect(info.baseLatency).toBeCloseTo(5.0, 1); // mock å€¼
            expect(info.outputLatency).toBeCloseTo(10.0, 1); // mock å€¼
            expect(info.totalLatency).toBeGreaterThan(0);
        });

        it('should calculate latency for ScriptProcessor mode', async () => {
            audioIO.configure({ useWorklet: false, bufferSize: 2048 });
            await audioIO.start();

            const info = audioIO.getLatencyInfo();
            expect(info.mode).toBe('script-processor');
            expect(info.bufferSize).toBe(2048);

            // bufferLatency = (2048 / 44100) * 1000 â‰ˆ 46.4ms
            expect(info.bufferLatency).toBeCloseTo(46.4, 1);
        });

        it('should return values with 2 decimal places', async () => {
            audioIO.configure({ useWorklet: true });
            await audioIO.start();

            const info = audioIO.getLatencyInfo();
            expect(info.bufferLatency.toString()).toMatch(/^\d+\.\d{1,2}$/);
            expect(info.totalLatency.toString()).toMatch(/^\d+\.\d{1,2}$/);
        });
    });

    // ==================== æ€§èƒ½ç»Ÿè®¡ ====================

    describe('getStats()', () => {
        it('should return copy of stats object', () => {
            const stats = audioIO.getStats();
            expect(stats).toEqual(audioIO.stats);
            expect(stats).not.toBe(audioIO.stats); // ä¸æ˜¯åŒä¸€ä¸ªå¼•ç”¨
        });

        it('should include all stat fields', () => {
            const stats = audioIO.getStats();
            expect(stats).toHaveProperty('framesProcessed');
            expect(stats).toHaveProperty('lastFrameTime');
            expect(stats).toHaveProperty('avgProcessingTime');
            expect(stats).toHaveProperty('dropouts');
        });
    });

    describe('_updateStats()', () => {
        it('should increment framesProcessed', () => {
            audioIO._updateStats(10);
            expect(audioIO.stats.framesProcessed).toBe(1);

            audioIO._updateStats(10);
            expect(audioIO.stats.framesProcessed).toBe(2);
        });

        it('should update lastFrameTime', () => {
            const mockNow = 1000;
            vi.spyOn(performance, 'now').mockReturnValue(mockNow);

            audioIO._updateStats(10);
            expect(audioIO.stats.lastFrameTime).toBe(mockNow);
        });

        it('should calculate moving average processing time', () => {
            audioIO._updateStats(10);
            expect(audioIO.stats.avgProcessingTime).toBeCloseTo(1.0, 1); // 0 * 0.9 + 10 * 0.1 = 1

            audioIO._updateStats(20);
            expect(audioIO.stats.avgProcessingTime).toBeCloseTo(2.9, 1); // 1 * 0.9 + 20 * 0.1 = 2.9
        });

        it('should use alpha = 0.1 for smoothing', () => {
            audioIO.stats.avgProcessingTime = 100;
            audioIO._updateStats(0);

            // 100 * (1 - 0.1) + 0 * 0.1 = 90
            expect(audioIO.stats.avgProcessingTime).toBeCloseTo(90, 1);
        });
    });

    // ==================== ç”Ÿå‘½å‘¨æœŸæ–¹æ³• ====================

    describe('start()', () => {
        it('should initialize audioContext', async () => {
            await audioIO.start();

            expect(audioIO.audioContext).toBeInstanceOf(MockAudioContext);
            expect(audioIO.audioContext.sampleRate).toBe(44100);
            expect(audioIO.audioContext.state).toBe('running');
        });

        it('should request microphone access', async () => {
            await audioIO.start();

            expect(mockGetUserMedia).toHaveBeenCalledWith({
                audio: {
                    echoCancellation: false,
                    noiseSuppression: false,
                    autoGainControl: false,
                    latency: 0
                },
                video: false
            });
        });

        it('should create source node from microphone stream', async () => {
            await audioIO.start();

            expect(audioIO.sourceNode).toBeDefined();
            expect(audioIO.stream).toBeInstanceOf(MockMediaStream);
        });

        it('should set isRunning and isInitialized to true', async () => {
            await audioIO.start();

            expect(audioIO.isRunning).toBe(true);
            expect(audioIO.isInitialized).toBe(true);
        });

        it('should choose worklet mode when supported and enabled', async () => {
            audioIO.configure({ useWorklet: true });
            await audioIO.start();

            expect(audioIO.mode).toBe('worklet');
        });

        it('should choose script-processor mode when worklet disabled', async () => {
            audioIO.configure({ useWorklet: false });
            await audioIO.start();

            expect(audioIO.mode).toBe('script-processor');
        });

        it('should return latency info', async () => {
            const result = await audioIO.start();

            expect(result).toHaveProperty('mode');
            expect(result).toHaveProperty('bufferLatency');
            expect(result).toHaveProperty('totalLatency');
        });

        it('should trigger state change callback with "started"', async () => {
            const callback = vi.fn();
            audioIO.onStateChange(callback);

            await audioIO.start();

            expect(callback).toHaveBeenCalledWith('started', expect.any(Object));
        });

        it('should warn if already running', async () => {
            const warnSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});

            await audioIO.start();
            await audioIO.start(); // ç¬¬äºŒæ¬¡è°ƒç”¨

            expect(warnSpy).toHaveBeenCalledWith(
                expect.stringContaining('å·²åœ¨è¿è¡Œ')
            );

            warnSpy.mockRestore();
        });

        it('should throw and trigger error callback on failure', async () => {
            const errorCallback = vi.fn();
            audioIO.onError(errorCallback);

            // Mock getUserMedia å¤±è´¥
            mockGetUserMedia.mockRejectedValueOnce(new Error('Permission denied'));

            await expect(audioIO.start()).rejects.toThrow('Permission denied');
            expect(errorCallback).toHaveBeenCalledWith('start', expect.any(Error));
        });
    });

    describe('stop()', () => {
        beforeEach(async () => {
            await audioIO.start();
        });

        it('should disconnect processor node', () => {
            const processorNode = audioIO.processorNode;
            audioIO.stop();

            expect(processorNode.disconnect).toHaveBeenCalled();
            expect(audioIO.processorNode).toBeNull();
        });

        it('should disconnect source node', () => {
            const sourceNode = audioIO.sourceNode;
            audioIO.stop();

            expect(sourceNode.disconnect).toHaveBeenCalled();
            expect(audioIO.sourceNode).toBeNull();
        });

        it('should stop all media tracks', () => {
            const track = audioIO.stream.getAudioTracks()[0];
            audioIO.stop();

            expect(track.stop).toHaveBeenCalled();
            expect(audioIO.stream).toBeNull();
        });

        it('should set isRunning to false', () => {
            audioIO.stop();
            expect(audioIO.isRunning).toBe(false);
        });

        it('should trigger state change callback with "stopped"', () => {
            const callback = vi.fn();
            audioIO.onStateChange(callback);

            audioIO.stop();

            expect(callback).toHaveBeenCalledWith('stopped', null);
        });

        it('should warn if not running', () => {
            const warnSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});

            audioIO.stop();
            audioIO.stop(); // ç¬¬äºŒæ¬¡è°ƒç”¨

            expect(warnSpy).toHaveBeenCalledWith(
                expect.stringContaining('æœªè¿è¡Œ')
            );

            warnSpy.mockRestore();
        });

        it('should clear ScriptProcessor callback in script-processor mode', async () => {
            // é‡æ–°åˆ›å»ºï¼Œä½¿ç”¨ ScriptProcessor æ¨¡å¼
            await audioIO.destroy();
            audioIO = new AudioIO();
            audioIO.configure({ useWorklet: false });
            await audioIO.start();

            const processor = audioIO.processorNode;
            processor.onaudioprocess = vi.fn();

            audioIO.stop();

            expect(processor.onaudioprocess).toBeNull();
        });
    });

    describe('destroy()', () => {
        it('should call stop() first', async () => {
            await audioIO.start();
            const stopSpy = vi.spyOn(audioIO, 'stop');

            await audioIO.destroy();

            expect(stopSpy).toHaveBeenCalled();
        });

        it('should close audioContext', async () => {
            await audioIO.start();
            const context = audioIO.audioContext;
            const closeSpy = vi.spyOn(context, 'close');

            await audioIO.destroy();

            expect(closeSpy).toHaveBeenCalled();
            expect(audioIO.audioContext).toBeNull();
        });

        it('should set isInitialized to false', async () => {
            await audioIO.start();
            await audioIO.destroy();

            expect(audioIO.isInitialized).toBe(false);
        });

        it('should handle destroy when not initialized', async () => {
            await expect(audioIO.destroy()).resolves.toBeUndefined();
        });
    });

    // ==================== Worklet é…ç½®åºåˆ—åŒ– ====================

    describe('_serializeConfigForWorklet()', () => {
        it('should use fallback defaults when appConfig is null', () => {
            audioIO.audioContext = new MockAudioContext();
            const config = audioIO._serializeConfigForWorklet();

            expect(config.sampleRate).toBe(44100);
            expect(config.algorithm).toBe('YIN');
            expect(config.clarityThreshold).toBe(0.85);
            expect(config.minFrequency).toBe(80);
            expect(config.maxFrequency).toBe(800);
            expect(config.minVolumeThreshold).toBe(0.01);
        });

        it('should map appConfig to worklet config', () => {
            audioIO.audioContext = new MockAudioContext();
            audioIO.appConfig = {
                pitchDetector: {
                    clarityThreshold: 0.9,
                    minFrequency: 100,
                    maxFrequency: 1000,
                    minVolumeThreshold: 0.005
                },
                smoothing: {
                    volume: { alpha: 0.5 },
                    brightness: { alpha: 0.6 }
                },
                onset: {
                    energyThreshold: 5,
                    silenceThreshold: -30,
                    attackDuration: 100
                }
            };

            const config = audioIO._serializeConfigForWorklet();

            expect(config.clarityThreshold).toBe(0.9);
            expect(config.minFrequency).toBe(100);
            expect(config.maxFrequency).toBe(1000);
            expect(config.minVolumeThreshold).toBe(0.005);
            expect(config.volumeAlpha).toBe(0.5);
            expect(config.brightnessAlpha).toBe(0.6);
            expect(config.energyThreshold).toBe(5);
            expect(config.silenceThreshold).toBe(-30);
            expect(config.minStateDuration).toBe(100);
        });

        it('should use default values for missing appConfig fields', () => {
            audioIO.audioContext = new MockAudioContext();
            audioIO.appConfig = {}; // ç©ºé…ç½®

            const config = audioIO._serializeConfigForWorklet();

            expect(config.clarityThreshold).toBe(0.85);
            expect(config.minVolumeThreshold).toBe(0.002);
            expect(config.volumeAlpha).toBe(0.3);
            expect(config.energyThreshold).toBe(3);
        });

        it('should include profiling flag when enabled', () => {
            audioIO.audioContext = new MockAudioContext();

            // æ¨¡æ‹Ÿ window å¯¹è±¡å­˜åœ¨
            const originalWindow = globalThis.window;
            globalThis.window = { __ENABLE_LATENCY_PROFILER__: true };

            const config = audioIO._serializeConfigForWorklet();

            expect(config.enableProfiling).toBe(true);

            // æ¢å¤
            if (originalWindow === undefined) {
                delete globalThis.window;
            } else {
                globalThis.window = originalWindow;
            }
        });

        it('should set profiling flag to false when not enabled', () => {
            audioIO.audioContext = new MockAudioContext();

            // æ¨¡æ‹Ÿ window å¯¹è±¡å­˜åœ¨ä½†æœªå¯ç”¨ profiling
            const originalWindow = globalThis.window;
            globalThis.window = {};

            const config = audioIO._serializeConfigForWorklet();

            expect(config.enableProfiling).toBe(false);

            // æ¢å¤
            if (originalWindow === undefined) {
                delete globalThis.window;
            } else {
                globalThis.window = originalWindow;
            }
        });
    });

    // ==================== Worklet æ¶ˆæ¯å¤„ç† ====================

    describe('_handleWorkletMessage()', () => {
        beforeEach(() => {
            audioIO.config.debug = false; // å…³é—­è°ƒè¯•æ—¥å¿—
        });

        it('should handle "ready" message', () => {
            const logSpy = vi.spyOn(console, 'log').mockImplementation(() => {});

            audioIO._handleWorkletMessage({
                data: {
                    type: 'ready',
                    data: { sampleRate: 44100 }
                }
            });

            expect(logSpy).toHaveBeenCalledWith(
                expect.stringContaining('Worklet å·²å°±ç»ª'),
                44100
            );

            logSpy.mockRestore();
        });

        it('should handle "pitch-detected" message', () => {
            const callback = vi.fn();
            audioIO.onPitchDetected(callback);

            const pitchData = { frequency: 440, clarity: 0.95 };
            audioIO._handleWorkletMessage({
                data: {
                    type: 'pitch-detected',
                    data: pitchData
                }
            });

            expect(callback).toHaveBeenCalledWith(pitchData);
            expect(audioIO.stats.pitchDetections).toBe(1);
        });

        it('should handle "pitch-frame" message with dedicated callback', () => {
            const callback = vi.fn();
            audioIO.onWorkletPitchFrame(callback);

            const pitchFrame = {
                frequency: 440,
                note: 'A4',
                clarity: 0.95,
                volume: 0.8
            };

            audioIO._handleWorkletMessage({
                data: {
                    type: 'pitch-frame',
                    data: pitchFrame,
                    timestamp: 1234.5
                }
            });

            expect(callback).toHaveBeenCalledWith(pitchFrame, 1234.5);
            expect(audioIO.stats.pitchDetections).toBe(1);
        });

        it('should fallback to onFrame for pitch-frame when dedicated callback not set', () => {
            const frameCallback = vi.fn();
            audioIO.onFrame(frameCallback);

            const warnSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});

            const pitchFrame = { frequency: 440 };
            audioIO._handleWorkletMessage({
                data: {
                    type: 'pitch-frame',
                    data: pitchFrame,
                    timestamp: 1234.5
                }
            });

            expect(warnSpy).toHaveBeenCalledWith(
                expect.stringContaining('æœªæ³¨å†Œä¸“ç”¨å›žè°ƒ')
            );
            expect(frameCallback).toHaveBeenCalledWith(pitchFrame, 1234.5);

            warnSpy.mockRestore();
        });

        it('should use performance.now() as fallback timestamp', () => {
            const callback = vi.fn();
            audioIO.onWorkletPitchFrame(callback);

            const mockNow = 5678.9;
            vi.spyOn(performance, 'now').mockReturnValue(mockNow);

            audioIO._handleWorkletMessage({
                data: {
                    type: 'pitch-frame',
                    data: { frequency: 440 },
                    timestamp: undefined // æ²¡æœ‰ timestamp
                }
            });

            expect(callback).toHaveBeenCalledWith(
                expect.any(Object),
                mockNow
            );
        });

        it('should handle "no-pitch" message in debug mode', () => {
            audioIO.config.debug = true;
            const logSpy = vi.spyOn(console, 'log').mockImplementation(() => {});

            audioIO._handleWorkletMessage({
                data: {
                    type: 'no-pitch',
                    data: { volume: 0.01 }
                }
            });

            expect(logSpy).toHaveBeenCalledWith(
                expect.stringContaining('æœªæ£€æµ‹åˆ°éŸ³é«˜'),
                0.01
            );

            logSpy.mockRestore();
        });

        it('should handle "error" message', () => {
            const errorCallback = vi.fn();
            audioIO.onError(errorCallback);

            audioIO._handleWorkletMessage({
                data: {
                    type: 'error',
                    data: { message: 'Worklet error' }
                }
            });

            expect(errorCallback).toHaveBeenCalledWith(
                'worklet',
                expect.objectContaining({ message: 'Worklet error' })
            );
        });

        it('should handle "stats" message and merge with local stats', () => {
            audioIO.config.debug = true;

            const workletStats = {
                processCount: 100,
                avgLatency: 2.5
            };

            audioIO._handleWorkletMessage({
                data: {
                    type: 'stats',
                    data: workletStats
                }
            });

            expect(audioIO.stats.workletStats).toEqual(workletStats);
        });

        it('should handle "config-applied" message', () => {
            const logSpy = vi.spyOn(console, 'log').mockImplementation(() => {});

            audioIO._handleWorkletMessage({
                data: { type: 'config-applied' }
            });

            expect(logSpy).toHaveBeenCalledWith(
                expect.stringContaining('é…ç½®å·²åº”ç”¨')
            );

            logSpy.mockRestore();
        });

        it('should handle unknown message types in debug mode', () => {
            audioIO.config.debug = true;
            const logSpy = vi.spyOn(console, 'log').mockImplementation(() => {});

            audioIO._handleWorkletMessage({
                data: {
                    type: 'custom-message',
                    data: { value: 123 }
                }
            });

            expect(logSpy).toHaveBeenCalledWith(
                expect.stringContaining('Worklet æ¶ˆæ¯'),
                'custom-message',
                { value: 123 }
            );

            logSpy.mockRestore();
        });
    });

    // ==================== é”™è¯¯å¤„ç† ====================

    describe('Error Handling', () => {
        beforeEach(() => {
            // ç¡®ä¿ mock åœ¨æ¯ä¸ªæµ‹è¯•å‰é‡ç½®
            vi.clearAllMocks();
            mockGetUserMedia.mockResolvedValue(new MockMediaStream());
        });

        it('should handle getUserMedia NotAllowedError', async () => {
            const error = new Error('User denied permission');
            error.name = 'NotAllowedError';
            mockGetUserMedia.mockRejectedValueOnce(error);

            await expect(audioIO.start()).rejects.toThrow(/éº¦å…‹é£Žæƒé™è¢«æ‹’ç»/);
        });

        it('should handle getUserMedia NotFoundError', async () => {
            const error = new Error('No device found');
            error.name = 'NotFoundError';
            mockGetUserMedia.mockRejectedValueOnce(error);

            await expect(audioIO.start()).rejects.toThrow(/æœªæ‰¾åˆ°éº¦å…‹é£Žè®¾å¤‡/);
        });

        it('should handle getUserMedia NotReadableError', async () => {
            const error = new Error('Device in use');
            error.name = 'NotReadableError';
            mockGetUserMedia.mockRejectedValueOnce(error);

            await expect(audioIO.start()).rejects.toThrow(/æ— æ³•è¯»å–éº¦å…‹é£Žæ•°æ®/);
        });

        it('should fallback to default config on OverconstrainedError', async () => {
            const error = new Error('Constraints not satisfied');
            error.name = 'OverconstrainedError';

            // ç¬¬ä¸€æ¬¡è°ƒç”¨å¤±è´¥ï¼Œç¬¬äºŒæ¬¡æˆåŠŸ
            mockGetUserMedia
                .mockRejectedValueOnce(error)
                .mockResolvedValueOnce(new MockMediaStream());

            await audioIO.start();

            // åº”è¯¥è°ƒç”¨äº†ä¸¤æ¬¡ getUserMedia
            expect(mockGetUserMedia).toHaveBeenCalledTimes(2);
            expect(mockGetUserMedia).toHaveBeenLastCalledWith({ audio: true });
        });

        it('should throw if fallback also fails on OverconstrainedError', async () => {
            const error1 = new Error('Constraints not satisfied');
            error1.name = 'OverconstrainedError';
            const error2 = new Error('Still failed');

            mockGetUserMedia
                .mockRejectedValueOnce(error1)
                .mockRejectedValueOnce(error2);

            await expect(audioIO.start()).rejects.toThrow(/ä¸æ”¯æŒæ‰€éœ€çš„éŸ³é¢‘é…ç½®/);
        });

        it('should handle callback errors gracefully', () => {
            const errorSpy = vi.spyOn(console, 'error').mockImplementation(() => {});

            audioIO.onStateChangeCallback = () => {
                throw new Error('Callback error');
            };

            // ä¸åº”è¯¥æŠ›å‡ºå¼‚å¸¸
            expect(() => audioIO._notifyStateChange('test', {})).not.toThrow();
            expect(errorSpy).toHaveBeenCalledWith(
                expect.stringContaining('çŠ¶æ€å˜åŒ–å›žè°ƒé”™è¯¯'),
                expect.any(Error)
            );

            errorSpy.mockRestore();
        });

        it('should handle error callback errors gracefully', () => {
            const errorSpy = vi.spyOn(console, 'error').mockImplementation(() => {});

            audioIO.onErrorCallback = () => {
                throw new Error('Error callback error');
            };

            expect(() => audioIO._notifyError('test', new Error('test'))).not.toThrow();
            expect(errorSpy).toHaveBeenCalledWith(
                expect.stringContaining('é”™è¯¯å›žè°ƒæœ¬èº«å‡ºé”™'),
                expect.any(Error)
            );

            errorSpy.mockRestore();
        });
    });

    // ==================== è¾¹ç•Œæƒ…å†µ ====================

    describe('Edge Cases', () => {
        it('should handle multiple configure() calls', () => {
            audioIO.configure({ bufferSize: 1024 });
            audioIO.configure({ sampleRate: 48000 });
            audioIO.configure({ debug: true });

            expect(audioIO.config.bufferSize).toBe(1024);
            expect(audioIO.config.sampleRate).toBe(48000);
            expect(audioIO.config.debug).toBe(true);
        });

        it('should handle stop() before start()', () => {
            const warnSpy = vi.spyOn(console, 'warn').mockImplementation(() => {});

            expect(() => audioIO.stop()).not.toThrow();
            expect(warnSpy).toHaveBeenCalled();

            warnSpy.mockRestore();
        });

        it('should handle destroy() before start()', async () => {
            await expect(audioIO.destroy()).resolves.toBeUndefined();
        });

        it('should handle missing AudioContext support', async () => {
            // Mock _initializeAudioContext æ–¹æ³•æ¥æŠ›å‡ºé”™è¯¯
            vi.spyOn(audioIO, '_initializeAudioContext').mockRejectedValueOnce(
                new Error('æµè§ˆå™¨ä¸æ”¯æŒ Web Audio API')
            );

            await expect(audioIO.start()).rejects.toThrow(/ä¸æ”¯æŒ Web Audio API/);
        });

        it('should handle missing mediaDevices support', async () => {
            // Mock _requestMicrophone æ–¹æ³•æ¥æŠ›å‡ºé”™è¯¯
            vi.spyOn(audioIO, '_requestMicrophone').mockRejectedValueOnce(
                new Error('æµè§ˆå™¨ä¸æ”¯æŒéº¦å…‹é£Žè®¿é—®\n\nè¯·ç¡®è®¤:\nâ€¢ ä½¿ç”¨çŽ°ä»£æµè§ˆå™¨ (Chrome 66+, Firefox 76+, Safari 14.1+)\nâ€¢ ä½¿ç”¨ HTTPS è¿žæŽ¥æˆ– localhost çŽ¯å¢ƒ')
            );

            await expect(audioIO.start()).rejects.toThrow(/ä¸æ”¯æŒéº¦å…‹é£Žè®¿é—®/);
        });

        it('should resume suspended AudioContext', async () => {
            const context = new MockAudioContext();
            context.state = 'suspended';
            const resumeSpy = vi.spyOn(context, 'resume');

            // Mock _initializeAudioContext æ¥è¿”å›ž suspended context
            vi.spyOn(audioIO, '_initializeAudioContext').mockImplementationOnce(async function() {
                this.audioContext = context;
                if (this.audioContext.state === 'suspended') {
                    await this.audioContext.resume();
                }
            });

            await audioIO.start();

            expect(resumeSpy).toHaveBeenCalled();
            expect(context.state).toBe('running');
        });
    });
});
